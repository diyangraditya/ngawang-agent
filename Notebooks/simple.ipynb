{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FIRST TIMER AGENTIC AI",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangSmith\n",
    "\n",
    "for model, chain, agent maintaining and inspection"
   ],
   "id": "c178e263e9780ce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:25:16.866407Z",
     "start_time": "2025-02-14T01:25:13.013643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ],
   "id": "7df9140457caed5e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tavily\n",
    "\n",
    "as a search engine tool"
   ],
   "id": "cef55d5e40ccade6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:28:39.659916Z",
     "start_time": "2025-02-14T01:28:36.924755Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()",
   "id": "d3a0868067a895a4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining tools\n",
    "\n",
    "We first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We have a built-in tool in LangChain to easily use Tavily search engine as tool.\n",
    "\n"
   ],
   "id": "87ebd487936ab4b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:28:45.843203Z",
     "start_time": "2025-02-14T01:28:41.682985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search_results = search.invoke(\"what is the weather in SF\")\n",
    "print(search_results)\n",
    "# If we want, we can create other tools.\n",
    "# Once we have all the tools we want, we can put them in a list that we will reference later.\n",
    "tools = [search]\n",
    "\n"
   ],
   "id": "bf92ad3b6e18ceff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739496524, 'localtime': '2025-02-13 17:28'}, 'current': {'last_updated_epoch': 1739495700, 'last_updated': '2025-02-13 17:15', 'temp_c': 13.9, 'temp_f': 57.0, 'is_day': 1, 'condition': {'text': 'Mist', 'icon': '//cdn.weatherapi.com/weather/64x64/day/143.png', 'code': 1030}, 'wind_mph': 25.5, 'wind_kph': 41.0, 'wind_degree': 227, 'wind_dir': 'SW', 'pressure_mb': 1003.0, 'pressure_in': 29.62, 'precip_mm': 3.6, 'precip_in': 0.14, 'humidity': 83, 'cloud': 100, 'feelslike_c': 11.1, 'feelslike_f': 52.1, 'windchill_c': 9.4, 'windchill_f': 48.9, 'heatindex_c': 12.2, 'heatindex_f': 54.0, 'dewpoint_c': 11.5, 'dewpoint_f': 52.6, 'vis_km': 9.7, 'vis_miles': 6.0, 'uv': 0.0, 'gust_mph': 38.3, 'gust_kph': 61.6}}\"}, {'url': 'https://world-weather.info/forecast/usa/san_francisco/february-2025/', 'content': \"pillar_safe Weather in San Francisco in February 2025 (California) - Detailed Weather Forecast for a Month Weather World Weather in San Francisco Weather in San Francisco in February 2025 San Francisco Weather Forecast for February 2025, is based on previous years' statistical data. +59°+50° +59°+52° +59°+50° +61°+52° +59°+50° +61°+50° +61°+52° +63°+52° +61°+52° +61°+50° +61°+50° +61°+50° +59°+50° +59°+50° +61°+50° +61°+52° +59°+50° +59°+48° +57°+48° +59°+50° +59°+48° +59°+50° +57°+46° +61°+50° +61°+50° +59°+50° +59°+48° +59°+50° Extended weather forecast in San Francisco HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Washington, D.C.+41° Sacramento+55° Pleasanton+55° Redwood City+55° San Leandro+55° San Mateo+54° San Rafael+52° San Ramon+52° South San Francisco+54° Vallejo+50° Palo Alto+55° Pacifica+55° Berkeley+54° Castro Valley+55° Concord+52° Daly City+54° Noverd+52° Sign Hill+54° world's temperature today day day Temperature units\"}]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:29:36.181982Z",
     "start_time": "2025-02-14T01:29:34.222855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Using language models\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ],
   "id": "c5f85d4fe5099416",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "you can call the language model by passing in a list of messages. By default, the response is a `content` string",
   "id": "1b0e39277df4b470"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:52:38.069916Z",
     "start_time": "2025-02-14T01:52:37.504314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = model.invoke([HumanMessage(content=\"who is david alaba?\")])\n",
    "response.content"
   ],
   "id": "4974012d21ac6da9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"David Alaba is an Austrian professional footballer who plays as a left-back or left midfielder for Bayern Munich and the Austria national team. He is considered one of the best left-backs in the world, known for his exceptional defensive skills, vision, and attacking prowess.\\n\\nAlaba was born on June 24, 1992, in Vienna, Austria. He joined Bayern Munich's youth academy at the age of 10 and rose through the ranks to make his professional debut for the club in 2010. He quickly established himself as a key player for Bayern, winning numerous titles including eight Bundesliga championships, four DFB-Pokals, and the 2020 UEFA Champions League.\\n\\nAt the international level, Alaba has been a mainstay for the Austria national team since his debut in 2009. He has appeared in several major tournaments, including the World Cup and the European Championship.\\n\\nAlaba is known for his versatility, ability to play in multiple positions, and his exceptional technical skills. He is also an attacking threat from the left flank, with a strong shot and vision for finding teammates in the box. In 2020, he was named to the UEFA Team of the Year and the FIFA FIFPro World11.\\n\\nAlaba has been linked with several top European clubs, including Real Madrid, Manchester United, and Paris Saint-Germain, and is considered one of the most sought-after defenders in the world. In 2021, he signed a five-year contract extension with Bayern Munich, which will keep him at the club until 2026.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools",
   "id": "c790d1e2b5d9c9e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:32:54.514732Z",
     "start_time": "2025-02-14T01:32:54.508934Z"
    }
   },
   "cell_type": "code",
   "source": "model_with_tools = model.bind_tools(tools)",
   "id": "d2f25ad09390dc9a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field.",
   "id": "e83742476ae419b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:33:11.311808Z",
     "start_time": "2025-02-14T01:33:10.032212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = model_with_tools.invoke([HumanMessage(content=\"give me a handphone recommendation for productivity?\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ],
   "id": "4079a8bd42cd9eb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: For a handphone recommendation for productivity, I'd suggest considering the following factors: screen size, battery life, multitasking capabilities, and app selection.\n",
      "\n",
      "Based on these factors, I'd recommend the Samsung Galaxy S22 Ultra or the Google Pixel 6 Pro. Both devices offer large screens, long battery life, and seamless multitasking capabilities.\n",
      "\n",
      "The Samsung Galaxy S22 Ultra features a massive 6.8-inch Dynamic AMOLED display, a large 5000mAh battery, and up to 16GB of RAM for smooth multitasking. It's also available with a range of storage options, including a microSD card slot.\n",
      "\n",
      "The Google Pixel 6 Pro, on the other hand, boasts a 6.7-inch OLED display, a 5124mAh battery, and 12GB of RAM. It's also known for its exceptional camera performance and timely software updates.\n",
      "\n",
      "Both devices are compatible with a wide range of productivity apps, including Microsoft Office, Google Workspace, and Todoist.\n",
      "\n",
      "Ultimately, the best handphone for productivity will depend on your individual needs and preferences. I recommend exploring both options and reading reviews to determine which device best fits your requirements.\n",
      "ToolCalls: []\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's try calling it with some input that would expect a tool to be called.",
   "id": "7d18acc61b42def7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:33:18.633253Z",
     "start_time": "2025-02-14T01:33:17.890813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = model_with_tools.invoke([HumanMessage(content=\"who is thomas alfa edinson\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")\n",
    "\n",
    "# sampe otw create agent"
   ],
   "id": "cc93d208ba7e9fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: \n",
      "ToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'Thomas Alfa Edison'}, 'id': 'call_h1m2', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that there's now no text content, but there is a tool call! It wants us to call the Tavily Search tool.\n",
    "\n",
    "This isn't calling that tool yet - it's just telling us to. In order to actually call it, we'll want to create our agent."
   ],
   "id": "f6b93bcef2acdbb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating Agent\n",
    "\n",
    "Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\n",
    "\n",
    "Now, we can initialize the agent with the LLM and the tools.\n",
    "\n",
    "Note that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood."
   ],
   "id": "85bd03f72ac47791"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:33:20.095418Z",
     "start_time": "2025-02-14T01:33:20.049942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(model, tools)"
   ],
   "id": "d612326e1b96d4f6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run the agent\n",
    "\n",
    "We can now run the agent with a few queries! Note that for now, these are all stateless queries (it won't remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\n",
    "\n",
    "First up, let's see how it responds when there's no need to call a tool:"
   ],
   "id": "2e0f18360d593f39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:33:23.761564Z",
     "start_time": "2025-02-14T01:33:22.259736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"hi!\")]})\n",
    "\n",
    "response[\"messages\"]"
   ],
   "id": "8fd9d3b4f08fd395",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}, id='78a8e01d-776d-4d90-b5ff-ea668ac52ffe'),\n",
       " AIMessage(content='Hi!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 1901, 'total_tokens': 1904, 'completion_time': 0.0025, 'prompt_time': 0.34279927, 'queue_time': -0.783156675, 'total_time': 0.34529927}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-0117deb9-64a5-47a4-a9f1-c2b8f10a6f37-0', usage_metadata={'input_tokens': 1901, 'output_tokens': 3, 'total_tokens': 1904})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T01:34:16.009480Z",
     "start_time": "2025-02-14T01:34:15.113887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n",
    ")\n",
    "response[\"messages\"]"
   ],
   "id": "ac0d93cc9c75b7ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='whats the weather in sf?', additional_kwargs={}, response_metadata={}, id='e671f072-934b-4af5-a7c4-a93ed28def78'),\n",
       " AIMessage(content='According to the latest data from the National Weather Service, the current weather in San Francisco is mostly cloudy with a high temperature of 63°F (17°C) and a low of 55°F (13°C). There is a 20% chance of light rain showers throughout the day.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 1912, 'total_tokens': 1971, 'completion_time': 0.049166667, 'prompt_time': 0.229291882, 'queue_time': -0.324496803, 'total_time': 0.278458549}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d3e97578-965f-4a86-bf54-1d43ed4425bb-0', usage_metadata={'input_tokens': 1912, 'output_tokens': 59, 'total_tokens': 1971})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Streaming messages\n",
    "\n",
    "We've seen how the agent can be called with .invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur."
   ],
   "id": "12d054b1380a8f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T02:46:47.396963Z",
     "start_time": "2025-02-14T02:46:39.892097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in agent_executor.stream(\n",
    "        {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ],
   "id": "f4b5634045167ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kjr7', 'function': {'arguments': '{\"query\":\"what is the current weather in san francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 1913, 'total_tokens': 1991, 'completion_time': 0.065, 'prompt_time': 0.343215887, 'queue_time': -2.395552903, 'total_time': 0.408215887}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-6fd1721f-45f5-4527-9606-3ef9afc462e8-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'what is the current weather in san francisco'}, 'id': 'call_kjr7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1913, 'output_tokens': 78, 'total_tokens': 1991})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1739501239, \\'localtime\\': \\'2025-02-13 18:47\\'}, \\'current\\': {\\'last_updated_epoch\\': 1739501100, \\'last_updated\\': \\'2025-02-13 18:45\\', \\'temp_c\\': 12.8, \\'temp_f\\': 55.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Light rain\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/296.png\\', \\'code\\': 1183}, \\'wind_mph\\': 23.3, \\'wind_kph\\': 37.4, \\'wind_degree\\': 235, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1004.0, \\'pressure_in\\': 29.66, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 100, \\'feelslike_c\\': 9.8, \\'feelslike_f\\': 49.7, \\'windchill_c\\': 9.6, \\'windchill_f\\': 49.2, \\'heatindex_c\\': 12.3, \\'heatindex_f\\': 54.2, \\'dewpoint_c\\': 11.4, \\'dewpoint_f\\': 52.5, \\'vis_km\\': 8.0, \\'vis_miles\\': 4.0, \\'uv\\': 0.0, \\'gust_mph\\': 35.1, \\'gust_kph\\': 56.5}}\"}, {\"url\": \"https://weathershogun.com/weather/usa/ca/san-francisco/480/february/2025-02-14\", \"content\": \"San Francisco, CA - Weather Forecast San Francisco, CA Today Tomorrow Hourly 7 days 30 days February San Francisco, California Weather: High Wind Warning (Strong winds expected that may cause property damage and pose a threat to life.) High Surf Warning (Issued for dangerously large waves and strong rip currents. Expect coastal flooding and hazardous swimming conditions.) Flood Watch (Flooding is possible in the near future; stay informed and prepared for potential evacuation or disruptions) Day 57° Night 41° Wind 15 mph 7 days 30 days Weather Forecast History Last Year\\'s Weather on This Day (February 14, 2024) Day 57° 52° Please note that while we strive for accuracy, the information provided may not always be correct.\"}]', name='tavily_search_results_json', id='39e83fb8-a3d5-4aa1-88fe-6574a365313c', tool_call_id='call_kjr7', artifact={'query': 'what is the current weather in san francisco', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in san francisco', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739501239, 'localtime': '2025-02-13 18:47'}, 'current': {'last_updated_epoch': 1739501100, 'last_updated': '2025-02-13 18:45', 'temp_c': 12.8, 'temp_f': 55.0, 'is_day': 0, 'condition': {'text': 'Light rain', 'icon': '//cdn.weatherapi.com/weather/64x64/night/296.png', 'code': 1183}, 'wind_mph': 23.3, 'wind_kph': 37.4, 'wind_degree': 235, 'wind_dir': 'SW', 'pressure_mb': 1004.0, 'pressure_in': 29.66, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 93, 'cloud': 100, 'feelslike_c': 9.8, 'feelslike_f': 49.7, 'windchill_c': 9.6, 'windchill_f': 49.2, 'heatindex_c': 12.3, 'heatindex_f': 54.2, 'dewpoint_c': 11.4, 'dewpoint_f': 52.5, 'vis_km': 8.0, 'vis_miles': 4.0, 'uv': 0.0, 'gust_mph': 35.1, 'gust_kph': 56.5}}\", 'score': 0.910038, 'raw_content': None}, {'url': 'https://weathershogun.com/weather/usa/ca/san-francisco/480/february/2025-02-14', 'title': 'Friday, February 14, 2025. San Francisco, CA - Weather Forecast', 'content': \"San Francisco, CA - Weather Forecast San Francisco, CA Today Tomorrow Hourly 7 days 30 days February San Francisco, California Weather: High Wind Warning (Strong winds expected that may cause property damage and pose a threat to life.) High Surf Warning (Issued for dangerously large waves and strong rip currents. Expect coastal flooding and hazardous swimming conditions.) Flood Watch (Flooding is possible in the near future; stay informed and prepared for potential evacuation or disruptions) Day 57° Night 41° Wind 15 mph 7 days 30 days Weather Forecast History Last Year's Weather on This Day (February 14, 2024) Day 57° 52° Please note that while we strive for accuracy, the information provided may not always be correct.\", 'score': 0.9049283, 'raw_content': None}], 'response_time': 1.84})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='The current weather in San Francisco is partly cloudy with a high of 55°F (12.8°C) and a low of 41°F (5°C). There is a Light rain condition with a wind speed of 23.3 mph (37.4 kph) coming from the Southwest. The humidity is 93% and the pressure is 1004.0 mb.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1620, 'total_tokens': 1699, 'completion_time': 0.065833333, 'prompt_time': 0.196262891, 'queue_time': 0.058327832999999996, 'total_time': 0.262096224}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-6abba431-de1d-43fb-b510-e566338f7925-0', usage_metadata={'input_tokens': 1620, 'output_tokens': 79, 'total_tokens': 1699})]}}\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Streaming Tokens\n",
    "\n",
    "In addition to streaming back messages, it is also useful to stream back tokens. We can do this with the .astream_events method."
   ],
   "id": "dbe4e8984d9c2283"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:05:17.645946Z",
     "start_time": "2025-02-14T03:05:16.202698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async for event in agent_executor.astream_events(\n",
    "        {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}, version=\"v1\"\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chain_start\":\n",
    "        if (\n",
    "                event[\"name\"] == \"Agent\"\n",
    "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "            print(\n",
    "                f\"Starting agent: {event['name']} with input: {event['data'].get('input')}\"\n",
    "            )\n",
    "    elif kind == \"on_chain_end\":\n",
    "        if (\n",
    "                event[\"name\"] == \"Agent\"\n",
    "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "            print()\n",
    "            print(\"--\")\n",
    "            print(\n",
    "                f\"Done agent: {event['name']} with output: {event['data'].get('output')['output']}\"\n",
    "            )\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")\n",
    "    elif kind == \"on_tool_start\":\n",
    "        print(\"--\")\n",
    "        print(\n",
    "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "        )\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Done tool: {event['name']}\")\n",
    "        print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "        print(\"--\")"
   ],
   "id": "9b800c8e3767dfac",
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAPIError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m agent_executor\u001B[38;5;241m.\u001B[39mastream_events(\n\u001B[1;32m      2\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: [HumanMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhats the weather in sf?\u001B[39m\u001B[38;5;124m\"\u001B[39m)]}, version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m ):\n\u001B[1;32m      4\u001B[0m     kind \u001B[38;5;241m=\u001B[39m event[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_chain_start\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/runnables/base.py:1381\u001B[0m, in \u001B[0;36mRunnable.astream_events\u001B[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001B[0m\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(msg)\n\u001B[1;32m   1380\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m aclosing(event_stream):\n\u001B[0;32m-> 1381\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m event_stream:\n\u001B[1;32m   1382\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m event\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/tracers/event_stream.py:781\u001B[0m, in \u001B[0;36m_astream_events_implementation_v1\u001B[0;34m(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001B[0m\n\u001B[1;32m    777\u001B[0m root_name \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, runnable\u001B[38;5;241m.\u001B[39mget_name())\n\u001B[1;32m    779\u001B[0m \u001B[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001B[39;00m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;66;03m# This arises because many of the argument types are unions\u001B[39;00m\n\u001B[0;32m--> 781\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m log \u001B[38;5;129;01min\u001B[39;00m _astream_log_implementation(  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    782\u001B[0m     runnable,\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    784\u001B[0m     config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[1;32m    785\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    786\u001B[0m     diff\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    787\u001B[0m     with_streamed_output_list\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    788\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    789\u001B[0m ):\n\u001B[1;32m    790\u001B[0m     run_log \u001B[38;5;241m=\u001B[39m run_log \u001B[38;5;241m+\u001B[39m log\n\u001B[1;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m encountered_start_event:\n\u001B[1;32m    793\u001B[0m         \u001B[38;5;66;03m# Yield the start event for the root runnable.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py:675\u001B[0m, in \u001B[0;36m_astream_log_implementation\u001B[0;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    673\u001B[0m     \u001B[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39msuppress(asyncio\u001B[38;5;241m.\u001B[39mCancelledError):\n\u001B[0;32m--> 675\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m task\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py:629\u001B[0m, in \u001B[0;36m_astream_log_implementation.<locals>.consume_astream\u001B[0;34m()\u001B[0m\n\u001B[1;32m    626\u001B[0m prev_final_output: Optional[Output] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    627\u001B[0m final_output: Optional[Output] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 629\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m runnable\u001B[38;5;241m.\u001B[39mastream(\u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    630\u001B[0m     prev_final_output \u001B[38;5;241m=\u001B[39m final_output\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m final_output \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2007\u001B[0m, in \u001B[0;36mPregel.astream\u001B[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# channel updates from step N are only visible in step N+1\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;66;03m# with channel updates applied only at the transition between steps\u001B[39;00m\n\u001B[1;32m   2006\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mtick(input_keys\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_channels):\n\u001B[0;32m-> 2007\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m runner\u001B[38;5;241m.\u001B[39matick(\n\u001B[1;32m   2008\u001B[0m         loop\u001B[38;5;241m.\u001B[39mtasks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m   2009\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_timeout,\n\u001B[1;32m   2010\u001B[0m         retry_policy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_policy,\n\u001B[1;32m   2011\u001B[0m         get_waiter\u001B[38;5;241m=\u001B[39mget_waiter,\n\u001B[1;32m   2012\u001B[0m     ):\n\u001B[1;32m   2013\u001B[0m         \u001B[38;5;66;03m# emit output\u001B[39;00m\n\u001B[1;32m   2014\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m output():\n\u001B[1;32m   2015\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m o\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langgraph/pregel/runner.py:444\u001B[0m, in \u001B[0;36mPregelRunner.atick\u001B[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001B[0m\n\u001B[1;32m    442\u001B[0m t \u001B[38;5;241m=\u001B[39m tasks[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 444\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m arun_with_retry(\n\u001B[1;32m    445\u001B[0m         t,\n\u001B[1;32m    446\u001B[0m         retry_policy,\n\u001B[1;32m    447\u001B[0m         stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_astream,\n\u001B[1;32m    448\u001B[0m         configurable\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    449\u001B[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001B[1;32m    450\u001B[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001B[1;32m    451\u001B[0m         },\n\u001B[1;32m    452\u001B[0m     )\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langgraph/pregel/retry.py:123\u001B[0m, in \u001B[0;36marun_with_retry\u001B[0;34m(task, retry_policy, stream, configurable)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[0;32m--> 123\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m task\u001B[38;5;241m.\u001B[39mproc\u001B[38;5;241m.\u001B[39mastream(task\u001B[38;5;241m.\u001B[39minput, config):\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;66;03m# if successful, end\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langgraph/utils/runnable.py:666\u001B[0m, in \u001B[0;36mRunnableSeq.astream\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    664\u001B[0m output: Any \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    665\u001B[0m add_supported \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 666\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m aiterator:\n\u001B[1;32m    667\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;66;03m# collect final output\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py:254\u001B[0m, in \u001B[0;36mLogStreamCallbackHandler.tap_output_aiter\u001B[0;34m(self, run_id, output)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtap_output_aiter\u001B[39m(\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28mself\u001B[39m, run_id: UUID, output: AsyncIterator[T]\n\u001B[1;32m    244\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AsyncIterator[T]:\n\u001B[1;32m    245\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Tap an output async iterator to stream its values to the log.\u001B[39;00m\n\u001B[1;32m    246\u001B[0m \n\u001B[1;32m    247\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;124;03m        T: The output value.\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 254\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m output:\n\u001B[1;32m    255\u001B[0m         \u001B[38;5;66;03m# root run is handled in .astream_log()\u001B[39;00m\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;66;03m# if we can't find the run silently ignore\u001B[39;00m\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;66;03m# eg. because this run wasn't included in the log\u001B[39;00m\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    259\u001B[0m             run_id \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_id\n\u001B[1;32m    260\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m (key \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_key_map_by_run_id\u001B[38;5;241m.\u001B[39mget(run_id))\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    269\u001B[0m             )\n\u001B[1;32m    270\u001B[0m         ):\n\u001B[1;32m    271\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/runnables/base.py:1447\u001B[0m, in \u001B[0;36mRunnable.atransform\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   1444\u001B[0m final: Input\n\u001B[1;32m   1445\u001B[0m got_first_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1447\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m ichunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28minput\u001B[39m:\n\u001B[1;32m   1448\u001B[0m     \u001B[38;5;66;03m# The default implementation of transform is to buffer input and\u001B[39;00m\n\u001B[1;32m   1449\u001B[0m     \u001B[38;5;66;03m# then call stream.\u001B[39;00m\n\u001B[1;32m   1450\u001B[0m     \u001B[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001B[39;00m\n\u001B[1;32m   1451\u001B[0m     \u001B[38;5;66;03m# the `+` operator.\u001B[39;00m\n\u001B[1;32m   1452\u001B[0m     \u001B[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001B[39;00m\n\u001B[1;32m   1453\u001B[0m     \u001B[38;5;66;03m# only operate on the last chunk,\u001B[39;00m\n\u001B[1;32m   1454\u001B[0m     \u001B[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001B[39;00m\n\u001B[1;32m   1455\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m got_first_val:\n\u001B[1;32m   1456\u001B[0m         final \u001B[38;5;241m=\u001B[39m ichunk\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/runnables/base.py:1447\u001B[0m, in \u001B[0;36mRunnable.atransform\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   1444\u001B[0m final: Input\n\u001B[1;32m   1445\u001B[0m got_first_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1447\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m ichunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28minput\u001B[39m:\n\u001B[1;32m   1448\u001B[0m     \u001B[38;5;66;03m# The default implementation of transform is to buffer input and\u001B[39;00m\n\u001B[1;32m   1449\u001B[0m     \u001B[38;5;66;03m# then call stream.\u001B[39;00m\n\u001B[1;32m   1450\u001B[0m     \u001B[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001B[39;00m\n\u001B[1;32m   1451\u001B[0m     \u001B[38;5;66;03m# the `+` operator.\u001B[39;00m\n\u001B[1;32m   1452\u001B[0m     \u001B[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001B[39;00m\n\u001B[1;32m   1453\u001B[0m     \u001B[38;5;66;03m# only operate on the last chunk,\u001B[39;00m\n\u001B[1;32m   1454\u001B[0m     \u001B[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001B[39;00m\n\u001B[1;32m   1455\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m got_first_val:\n\u001B[1;32m   1456\u001B[0m         final \u001B[38;5;241m=\u001B[39m ichunk\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/runnables/base.py:1447\u001B[0m, in \u001B[0;36mRunnable.atransform\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   1444\u001B[0m final: Input\n\u001B[1;32m   1445\u001B[0m got_first_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1447\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m ichunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28minput\u001B[39m:\n\u001B[1;32m   1448\u001B[0m     \u001B[38;5;66;03m# The default implementation of transform is to buffer input and\u001B[39;00m\n\u001B[1;32m   1449\u001B[0m     \u001B[38;5;66;03m# then call stream.\u001B[39;00m\n\u001B[1;32m   1450\u001B[0m     \u001B[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001B[39;00m\n\u001B[1;32m   1451\u001B[0m     \u001B[38;5;66;03m# the `+` operator.\u001B[39;00m\n\u001B[1;32m   1452\u001B[0m     \u001B[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001B[39;00m\n\u001B[1;32m   1453\u001B[0m     \u001B[38;5;66;03m# only operate on the last chunk,\u001B[39;00m\n\u001B[1;32m   1454\u001B[0m     \u001B[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001B[39;00m\n\u001B[1;32m   1455\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m got_first_val:\n\u001B[1;32m   1456\u001B[0m         final \u001B[38;5;241m=\u001B[39m ichunk\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/runnables/base.py:1012\u001B[0m, in \u001B[0;36mRunnable.astream\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    995\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mastream\u001B[39m(\n\u001B[1;32m    996\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    997\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[1;32m    998\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[1;32m   1000\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AsyncIterator[Output]:\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Default implementation of astream, which calls ainvoke.\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;124;03m    Subclasses should override this method if they support streaming output.\u001B[39;00m\n\u001B[1;32m   1003\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1010\u001B[0m \u001B[38;5;124;03m        The output of the Runnable.\u001B[39;00m\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1012\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mainvoke(\u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langgraph/utils/runnable.py:321\u001B[0m, in \u001B[0;36mRunnableCallable.ainvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    319\u001B[0m         ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mcreate_task(coro, context\u001B[38;5;241m=\u001B[39mcontext)\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 321\u001B[0m         ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m coro\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langgraph/prebuilt/chat_agent_executor.py:678\u001B[0m, in \u001B[0;36mcreate_react_agent.<locals>.acall_model\u001B[0;34m(state, config)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21macall_model\u001B[39m(state: AgentState, config: RunnableConfig) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AgentState:\n\u001B[1;32m    677\u001B[0m     _validate_chat_history(state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m--> 678\u001B[0m     response \u001B[38;5;241m=\u001B[39m cast(AIMessage, \u001B[38;5;28;01mawait\u001B[39;00m model_runnable\u001B[38;5;241m.\u001B[39mainvoke(state, config))\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;66;03m# add agent name to the AIMessage\u001B[39;00m\n\u001B[1;32m    680\u001B[0m     response\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/runnables/base.py:3060\u001B[0m, in \u001B[0;36mRunnableSequence.ainvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3058\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mcreate_task(part(), context\u001B[38;5;241m=\u001B[39mcontext)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   3059\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3060\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mcreate_task(part())\n\u001B[1;32m   3061\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3062\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/runnables/base.py:5364\u001B[0m, in \u001B[0;36mRunnableBindingBase.ainvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   5358\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mainvoke\u001B[39m(\n\u001B[1;32m   5359\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5360\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[1;32m   5361\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5362\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[1;32m   5363\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[0;32m-> 5364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39mainvoke(\n\u001B[1;32m   5365\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   5366\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[1;32m   5367\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[1;32m   5368\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:305\u001B[0m, in \u001B[0;36mBaseChatModel.ainvoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mainvoke\u001B[39m(\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    303\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    304\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m--> 305\u001B[0m     llm_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magenerate_prompt(\n\u001B[1;32m    306\u001B[0m         [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    307\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    308\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    309\u001B[0m         tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    310\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    311\u001B[0m         run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    312\u001B[0m         run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    313\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    314\u001B[0m     )\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ChatGeneration, llm_result\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:870\u001B[0m, in \u001B[0;36mBaseChatModel.agenerate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21magenerate_prompt\u001B[39m(\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    864\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    867\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    868\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    869\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magenerate(\n\u001B[1;32m    871\u001B[0m         prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    872\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:830\u001B[0m, in \u001B[0;36mBaseChatModel.agenerate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    817\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    818\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\n\u001B[1;32m    819\u001B[0m             \u001B[38;5;241m*\u001B[39m[\n\u001B[1;32m    820\u001B[0m                 run_manager\u001B[38;5;241m.\u001B[39mon_llm_end(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    828\u001B[0m             ]\n\u001B[1;32m    829\u001B[0m         )\n\u001B[0;32m--> 830\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    831\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    832\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item, union-attr]\u001B[39;00m\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    834\u001B[0m ]\n\u001B[1;32m    835\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:986\u001B[0m, in \u001B[0;36mBaseChatModel._agenerate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_stream(\n\u001B[1;32m    981\u001B[0m     async_api\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    982\u001B[0m     run_manager\u001B[38;5;241m=\u001B[39mrun_manager,\n\u001B[1;32m    983\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    984\u001B[0m ):\n\u001B[1;32m    985\u001B[0m     chunks: \u001B[38;5;28mlist\u001B[39m[ChatGenerationChunk] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 986\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_astream(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    987\u001B[0m         chunk\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mresponse_metadata \u001B[38;5;241m=\u001B[39m _gen_info_and_msg_metadata(chunk)\n\u001B[1;32m    988\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_manager:\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/langchain_groq/chat_models.py:552\u001B[0m, in \u001B[0;36mChatGroq._astream\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    549\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m}\n\u001B[1;32m    551\u001B[0m default_chunk_class: Type[BaseMessageChunk] \u001B[38;5;241m=\u001B[39m AIMessageChunk\n\u001B[0;32m--> 552\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masync_client\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m    553\u001B[0m     messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    554\u001B[0m ):\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunk, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    556\u001B[0m         chunk \u001B[38;5;241m=\u001B[39m chunk\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/groq/_streaming.py:147\u001B[0m, in \u001B[0;36mAsyncStream.__aiter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__aiter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AsyncIterator[_T]:\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator:\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m item\n",
      "File \u001B[0;32m~/miniconda3/envs/trial-agent/lib/python3.10/site-packages/groq/_streaming.py:193\u001B[0m, in \u001B[0;36mAsyncStream.__stream__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    190\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m message \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(message, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    191\u001B[0m                 message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred during streaming\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 193\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m APIError(\n\u001B[1;32m    194\u001B[0m                 message\u001B[38;5;241m=\u001B[39mmessage,\n\u001B[1;32m    195\u001B[0m                 request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mrequest,\n\u001B[1;32m    196\u001B[0m                 body\u001B[38;5;241m=\u001B[39mdata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    197\u001B[0m             )\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m process_data(data\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: data, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent\u001B[39m\u001B[38;5;124m\"\u001B[39m: sse\u001B[38;5;241m.\u001B[39mevent}, cast_to\u001B[38;5;241m=\u001B[39mcast_to, response\u001B[38;5;241m=\u001B[39mresponse)\n\u001B[1;32m    201\u001B[0m \u001B[38;5;66;03m# Ensure the entire stream is consumed\u001B[39;00m\n",
      "\u001B[0;31mAPIError\u001B[0m: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details."
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding in memory\n",
    "\n",
    "As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from)."
   ],
   "id": "a0a711d00a26cf70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:23:52.229120Z",
     "start_time": "2025-02-14T03:23:52.224984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memori = MemorySaver()"
   ],
   "id": "514b6d01bdaafecc",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:23:56.129456Z",
     "start_time": "2025-02-14T03:23:56.114933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_executor = create_react_agent(model, tools, checkpointer=memori)\n",
    "\n",
    "config = {\"configurable\" : {\"thread_id\" : \"abc123\"}}"
   ],
   "id": "6968273ad94d661d",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:24:16.345580Z",
     "start_time": "2025-02-14T03:24:14.504293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in agent_executor.stream(\n",
    "        {\"messages\": [HumanMessage(content=\"hi im bob!\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ],
   "id": "a545170d03715733",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hello Bob!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 1905, 'total_tokens': 1909, 'completion_time': 0.003333333, 'prompt_time': 0.239943398, 'queue_time': -0.29073663, 'total_time': 0.243276731}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ea0184d1-5721-48ac-8bf5-baa20caa0afa-0', usage_metadata={'input_tokens': 1905, 'output_tokens': 4, 'total_tokens': 1909})]}}\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:30:03.246946Z",
     "start_time": "2025-02-14T03:30:01.362649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in agent_executor.stream(\n",
    "        {\"messages\": [HumanMessage(content=\"do you remember my name?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ],
   "id": "b002a8a1de7aa6fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Yes, I remember that your name is Bob.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 1944, 'total_tokens': 1955, 'completion_time': 0.009166667, 'prompt_time': 0.342846021, 'queue_time': -0.403341719, 'total_time': 0.352012688}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-e649eef6-093a-4680-b22c-23aff985eada-0', usage_metadata={'input_tokens': 1944, 'output_tokens': 11, 'total_tokens': 1955})]}}\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "if wanna have a new convo, change the `thread_id` used",
   "id": "da72131f5166ef81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:37:50.215203Z",
     "start_time": "2025-02-14T03:37:49.254982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mengconfig = {\"configurable\" : {\"thread_id\" : \"diyrad\"}}\n",
    "\n",
    "for chunk in agent_executor.stream(\n",
    "        {\"messages\": [HumanMessage(content=\"what's my name?\")]}, mengconfig\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ],
   "id": "f17f22ebe878bd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"I'm not sure. I don't have any information about your name.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 1909, 'total_tokens': 1925, 'completion_time': 0.013333333, 'prompt_time': 0.245515131, 'queue_time': -0.297238971, 'total_time': 0.258848464}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-632d92aa-7dd3-4445-87e1-5791bf0f525a-0', usage_metadata={'input_tokens': 1909, 'output_tokens': 16, 'total_tokens': 1925})]}}\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## tamat....\n",
    "\n",
    "That's a wrap! In this quick start we covered how to create a simple agent. We've then shown how to stream back a response - not only with the intermediate steps, but also tokens! We've also added in memory so you can have a conversation with them. Agents are a complex topic with lots to learn!"
   ],
   "id": "29d34255c1fe275a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b827f8c8228547fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
